%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Scheduling}
%%%%%%%%%%%%%%%%%%%%%%%%%

%\todo[inline]{\\
%What is the job of the scheduler? \\
%What makes it difficult? \\
%What are the typical approaches? \\
%What are example scheduling algorithms? \\
%Input vs Output Queuing\\
%Work-conserving vs non-work-conserving\\
%}
In general terms, given a shared resource and a pool of ready candidates, scheduling consists of selecting one candidate from the pool to use the shared resource according to a selection rule (scheduling algorithm).  For example, in computing the candidates are jobs and the shared resource is the central processing unit (CPU).  Likewise in networks, the candidates are packets and the shared resource is available bandwidth or a vacant location in an aggregation queue.

Different scheduling algorithms are used to prioritize certain packet flows (Priority Queuing) or to allocate the available bandwidth between active flows (e.g., Weighted Fair Queuing) to deliver the expected service quality under various network congestion conditions.

A well designed scheduler must be work-conserving, i.e., make efficient use of the shared resource; it must not allow the resource to become idle if there are ready candidates.  In addition, scheduling decisions must be made with minimal latency with respect to any changes in the pool of ready candidates, i.e., if new candidates become ready, the scheduler must evaluate the next selection according to the selection rule with the most recent pool of candidates.

To illustrate these properties, let us consider two examples.
\paragraph{Work Conservation}
Consider a traffic manager (TM) with thousands of queues. A naively designed scheduler would test each queue in a loop to determine the pool of ready (non-empty) queues.  If a cycle of checking all the queues takes longer than the time to transmit a minimum-length packet, the output would become idle.  One solution would be to maintain a list of ready queues based on packet arrival and departure events.  Then the scheduler would consider only queues that are not empty, but it still must make the next queue selection within the minimum-length packet transmission time.
\paragraph{Scheduling Latency}
In a TM with hierarchical scheduling, if a scheduling cycle is triggered by the availability of output bandwidth at the final level and each level of scheduling is triggered by a vacancy at the input of the next-level scheduler, there may be significant latency between packet arrivals at the input  and the output of the hierarchical scheduler.  Let us refer to this approach as dequeue scheduling.  In such a system, if a high priority packet arrives at the input of the scheduler, it would take several intermediate scheduling decisions for it to reach the output.  A solution would be a mechanism that can "push" packets through the scheduling hierarchy based on packet arrivals.  We will call this approach enqueue scheduling.

The PIFO queue enables a scheduler to have both characteristics of work conservation and enqueue scheduling.





